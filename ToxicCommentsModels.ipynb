{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import *\n",
    "from keras.optimizers import * \n",
    "from keras.models import *\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '../ToxicComments/'\n",
    "train = pd.read_csv(path + 'toxic_train.csv')\n",
    "test = pd.read_csv(path + 'toxic_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = train['comment_text'].str.lower()\n",
    "y_train = train[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values\n",
    "\n",
    "x_test = test['comment_text'].str.lower()\n",
    "num_classes =6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will use GloVe embeddings first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have to get this ready - https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html i got the code from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary=100000\n",
    "\n",
    "maxlen=150\n",
    "embedding_size=300 #int(100000**0.25)+1 if its not pretrained\n",
    "#embedding vector dimension should be the 4th root of the vocabulary according to Google\n",
    "x_train_original = x_train\n",
    "y_train_original = y_train\n",
    "x_test_original = x_test\n",
    "token = text.Tokenizer(num_words=vocabulary,lower=True)\n",
    "token.fit_on_texts(list(x_train)+list(x_test))\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "x_test=token.texts_to_sequences(x_test)\n",
    "x_train=sequence.pad_sequences(x_train,maxlen=maxlen)\n",
    "x_test=sequence.pad_sequences(x_test,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(path+'glove.840B.300d.txt',encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = token.word_index\n",
    "num_words = min(vocabulary, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embedding_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= vocabulary:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dimiter\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "x_train_original_split, x_val_original_split, y_train_original_split,y_val_original_split=train_test_split(x_train, \n",
    "                                       y_train, \n",
    "                                       train_size=0.9, \n",
    "                                       random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary,\n",
    "                    embedding_size,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=maxlen,\n",
    "                    trainable=False))\n",
    "model.add(LSTM(256,return_sequences=True,dropout=0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=SGD(lr=1e-3),#Adam, Adagrad \n",
    "             metrics = ['accuracy'])\n",
    "#binary - because we have multilabeled samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dimiter\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(x_train, \n",
    "                                                                          y_train, \n",
    "                                                                          train_size=0.9, \n",
    "                                                                          random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 482s 3ms/step - loss: 0.2336 - acc: 0.9633 - val_loss: 0.1567 - val_acc: 0.9634\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 477s 3ms/step - loss: 0.1553 - acc: 0.9633 - val_loss: 0.1443 - val_acc: 0.9634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x9628a64518>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_split, y_train_split, validation_data=(x_val_split, y_val_split),\n",
    "          epochs=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with ConvLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary,\n",
    "                    embedding_size,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=maxlen,\n",
    "                    trainable=False))\n",
    "model.add(LSTM(256,return_sequences=True,dropout=0.5))\n",
    "model.add(Conv1D(128,5,activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=SGD(lr=1e-3),#Adam, Adagrad \n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 663s 5ms/step - loss: 0.4208 - acc: 0.9189 - val_loss: 0.2034 - val_acc: 0.9634\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 631s 4ms/step - loss: 0.1711 - acc: 0.9633 - val_loss: 0.1490 - val_acc: 0.9634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x9695cf9588>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_split, y_train_split, validation_data=(x_val_split, y_val_split),\n",
    "          epochs=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP(multilayer perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "143613/143613 [==============================] - 42s 293us/step - loss: 1.3698 - acc: 0.9137\n",
      "Epoch 2/20\n",
      "143613/143613 [==============================] - 6s 45us/step - loss: 0.6094 - acc: 0.9621\n",
      "Epoch 3/20\n",
      "143613/143613 [==============================] - 6s 42us/step - loss: 0.5971 - acc: 0.9629\n",
      "Epoch 4/20\n",
      "143613/143613 [==============================] - 6s 42us/step - loss: 0.5936 - acc: 0.9632\n",
      "Epoch 5/20\n",
      "143613/143613 [==============================] - 6s 41us/step - loss: 0.5923 - acc: 0.9632\n",
      "Epoch 6/20\n",
      "143613/143613 [==============================] - 6s 42us/step - loss: 0.5919 - acc: 0.9633\n",
      "Epoch 7/20\n",
      "143613/143613 [==============================] - 6s 40us/step - loss: 0.5917 - acc: 0.9633\n",
      "Epoch 8/20\n",
      "143613/143613 [==============================] - 6s 39us/step - loss: 0.5915 - acc: 0.9633\n",
      "Epoch 9/20\n",
      "143613/143613 [==============================] - 6s 44us/step - loss: 0.5913 - acc: 0.9633\n",
      "Epoch 10/20\n",
      "143613/143613 [==============================] - 6s 41us/step - loss: 0.5912 - acc: 0.9633\n",
      "Epoch 11/20\n",
      "143613/143613 [==============================] - 6s 41us/step - loss: 0.5911 - acc: 0.9633\n",
      "Epoch 12/20\n",
      "143613/143613 [==============================] - 6s 44us/step - loss: 0.5911 - acc: 0.9633\n",
      "Epoch 13/20\n",
      "143613/143613 [==============================] - 6s 42us/step - loss: 0.5911 - acc: 0.9633\n",
      "Epoch 14/20\n",
      "143613/143613 [==============================] - 6s 43us/step - loss: 0.5911 - acc: 0.9633\n",
      "Epoch 15/20\n",
      "143613/143613 [==============================] - 6s 44us/step - loss: 0.5911 - acc: 0.9633\n",
      "Epoch 16/20\n",
      "143613/143613 [==============================] - 6s 41us/step - loss: 0.5911 - acc: 0.9633\n",
      "Epoch 17/20\n",
      "143613/143613 [==============================] - 6s 40us/step - loss: 0.5911 - acc: 0.9633\n",
      "Epoch 18/20\n",
      "143613/143613 [==============================] - 5s 37us/step - loss: 0.5910 - acc: 0.9633\n",
      "Epoch 19/20\n",
      "143613/143613 [==============================] - 5s 37us/step - loss: 0.5911 - acc: 0.9633\n",
      "Epoch 20/20\n",
      "143613/143613 [==============================] - 6s 42us/step - loss: 0.5910 - acc: 0.9633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3d36ba8c18>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(64, input_dim=maxlen, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_original_split, y_train_original_split,\n",
    "          epochs=20,\n",
    "          batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary,\n",
    "                    embedding_size,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=maxlen,\n",
    "                    trainable=False))\n",
    "model.add(LSTM(32, return_sequences=True,))  \n",
    "model.add(LSTM(32, return_sequences=True))  \n",
    "model.add(LSTM(32)) \n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adagrad(lr=1e-3),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 2398s 17ms/step - loss: 0.1656 - acc: 0.9628\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 2436s 17ms/step - loss: 0.1045 - acc: 0.9663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x96f734dba8>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_split, y_train_split,\n",
    "          epochs=2,\n",
    "          batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary,\n",
    "                    embedding_size,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=maxlen,\n",
    "                    trainable=False))\n",
    "model.add(Conv1D(128,5,activation='relu'))\n",
    "model.add(Conv1D(128,5,activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(256,5,activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adagrad(lr=1e-3),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/10\n",
      "143613/143613 [==============================] - 52s 364us/step - loss: 0.0751 - acc: 0.9751 - val_loss: 0.0715 - val_acc: 0.9757\n",
      "Epoch 2/10\n",
      "143613/143613 [==============================] - 52s 362us/step - loss: 0.0724 - acc: 0.9758 - val_loss: 0.0694 - val_acc: 0.9763\n",
      "Epoch 3/10\n",
      "143613/143613 [==============================] - 52s 364us/step - loss: 0.0707 - acc: 0.9762 - val_loss: 0.0700 - val_acc: 0.9761\n",
      "Epoch 4/10\n",
      "143613/143613 [==============================] - 52s 364us/step - loss: 0.0694 - acc: 0.9767 - val_loss: 0.0681 - val_acc: 0.9765\n",
      "Epoch 5/10\n",
      "143613/143613 [==============================] - 52s 365us/step - loss: 0.0686 - acc: 0.9768 - val_loss: 0.0672 - val_acc: 0.9768\n",
      "Epoch 6/10\n",
      "143613/143613 [==============================] - 52s 360us/step - loss: 0.0679 - acc: 0.9771 - val_loss: 0.0673 - val_acc: 0.9768\n",
      "Epoch 7/10\n",
      "143613/143613 [==============================] - 52s 359us/step - loss: 0.0671 - acc: 0.9771 - val_loss: 0.0671 - val_acc: 0.9769\n",
      "Epoch 8/10\n",
      "143613/143613 [==============================] - 52s 359us/step - loss: 0.0664 - acc: 0.9774 - val_loss: 0.0676 - val_acc: 0.9765\n",
      "Epoch 9/10\n",
      "143613/143613 [==============================] - 52s 359us/step - loss: 0.0661 - acc: 0.9775 - val_loss: 0.0664 - val_acc: 0.9770\n",
      "Epoch 10/10\n",
      "143613/143613 [==============================] - 51s 359us/step - loss: 0.0652 - acc: 0.9778 - val_loss: 0.0663 - val_acc: 0.9769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x9754333898>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_split, y_train_split,\n",
    "          validation_data=(x_val_split,y_val_split),\n",
    "          epochs=10,\n",
    "          batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size_normal = int(vocabulary**0.25) +1\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary,\n",
    "                    embedding_size_normal,\n",
    "                    #weights=[embedding_matrix],\n",
    "                    input_length=maxlen,\n",
    "                    trainable=False))\n",
    "model.add(Conv1D(128,5,activation='relu'))\n",
    "model.add(Conv1D(128,5,activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(256,5,activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adagrad(lr=1e-3),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/10\n",
      "143613/143613 [==============================] - 36s 249us/step - loss: 0.1413 - acc: 0.9631 - val_loss: 0.1373 - val_acc: 0.9634\n",
      "Epoch 2/10\n",
      "143613/143613 [==============================] - 34s 237us/step - loss: 0.1360 - acc: 0.9633 - val_loss: 0.1371 - val_acc: 0.9634\n",
      "Epoch 3/10\n",
      "143613/143613 [==============================] - 35s 243us/step - loss: 0.1349 - acc: 0.9633 - val_loss: 0.1366 - val_acc: 0.9634\n",
      "Epoch 4/10\n",
      "143613/143613 [==============================] - 36s 247us/step - loss: 0.1342 - acc: 0.9633 - val_loss: 0.1353 - val_acc: 0.9634\n",
      "Epoch 5/10\n",
      "143613/143613 [==============================] - 35s 241us/step - loss: 0.1335 - acc: 0.9633 - val_loss: 0.1345 - val_acc: 0.9634\n",
      "Epoch 6/10\n",
      "143613/143613 [==============================] - 35s 243us/step - loss: 0.1333 - acc: 0.9633 - val_loss: 0.1331 - val_acc: 0.9634\n",
      "Epoch 7/10\n",
      "143613/143613 [==============================] - 34s 237us/step - loss: 0.1329 - acc: 0.9633 - val_loss: 0.1330 - val_acc: 0.9634\n",
      "Epoch 8/10\n",
      "143613/143613 [==============================] - 34s 236us/step - loss: 0.1327 - acc: 0.9633 - val_loss: 0.1331 - val_acc: 0.9634\n",
      "Epoch 9/10\n",
      "143613/143613 [==============================] - 35s 247us/step - loss: 0.1326 - acc: 0.9633 - val_loss: 0.1329 - val_acc: 0.9634\n",
      "Epoch 10/10\n",
      "143613/143613 [==============================] - 35s 244us/step - loss: 0.1324 - acc: 0.9633 - val_loss: 0.1325 - val_acc: 0.9634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x9764c4bf60>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_split, y_train_split,\n",
    "          validation_data=(x_val_split,y_val_split),\n",
    "          epochs=10,\n",
    "          batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - Basic Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_36 (Embedding)     (None, 150, 18)           1800000   \n",
      "_________________________________________________________________\n",
      "lstm_36 (LSTM)               (None, 150, 128)          75264     \n",
      "=================================================================\n",
      "Total params: 1,875,264\n",
      "Trainable params: 75,264\n",
      "Non-trainable params: 1,800,000\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-165e4413c7ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#model_merged.add(Concatenate([model_lstm,model_attention]))#Merge('concat') is now Concatenate([])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mmodel_merged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_attention\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_lstm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mul'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mmodel_merged\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAveragePooling1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mmodel_merged\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "embedding_size_normal = int(vocabulary**0.25)+1\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocabulary,\n",
    "                    embedding_size_normal,\n",
    "                    #weights=[embedding_matrix],\n",
    "                    input_length=maxlen,\n",
    "                    trainable=False))\n",
    "model_lstm.add(LSTM(128,return_sequences=True))\n",
    "model_lstm.summary()\n",
    "model_attention=model_lstm\n",
    "model_attention.add(TimeDistributed(Dense(1)))\n",
    "model_attention.add(Flatten())\n",
    "model_attention.add(Activation('sigmoid'))\n",
    "model_attention.add(RepeatVector(128))\n",
    "model_attention.add(Permute((2,1)))\n",
    "\n",
    "#model_merged.add(Concatenate([model_lstm,model_attention]))#Merge('concat') is now Concatenate([])\n",
    "model_merged = merge([model_attention,model_lstm],mode='mul')\n",
    "model_merged.add(Flatten())\n",
    "model_merged.add(Dense(num_classes,activation='sigmoid'))\n",
    "\n",
    "model_merged.compile(loss='binary_crossentropy',\n",
    "                    optimizer=Adam(lr=1e-3),\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train_split, y_train_split,\n",
    "          validation_data=(x_val_split,y_val_split),\n",
    "          epochs=10,\n",
    "          batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
